{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":127612,"sourceType":"datasetVersion","datasetId":64826},{"sourceId":11440436,"sourceType":"datasetVersion","datasetId":7166601},{"sourceId":11440672,"sourceType":"datasetVersion","datasetId":7166760}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üè• MedBot: A GenAI-Powered Assistant for Classifying and Responding to Medical Transcriptions\n\n## üìò Project Overview\n\nThis project demonstrates how **Generative AI techniques** can be combined to build a real-world assistant for healthcare support. The assistant, called **MedBot**, is designed to understand medical case descriptions, classify them into medical specialties, and offer relevant next steps‚Äîsuch as retrieving similar records or suggesting nearby providers.\n\nThe solution showcases both **model fine-tuning** and **tool-augmented conversation design** using **LangGraph** and **Gemini API**.\n\n---\n\n## üéØ Objective\n\nBuild a domain-specific GenAI assistant that:\n\n- Understands patient summaries or transcriptions\n- Classifies the case into **Urology**, **Nephrology**, or **Other**\n- Responds based on the category with context-aware suggestions\n\n\n---\n\n## üí° Generative AI Techniques Used\n\n| Technique                      | Purpose                                                                 |\n|-------------------------------|-------------------------------------------------------------------------|\n| **Prompt Engineering**         | Establish a baseline zero-shot classifier using Gemini                 |\n| **Gemini API Fine-Tuning**     | Train a custom model on labeled medical transcriptions                 |\n| **Semantic Embedding Evaluation** | Evaluate predictions via similarity to reference examples           |\n| **LangGraph Tool Routing**     | Route between classifier, search, and user interaction nodes           |\n| **Tool-Augmented Reasoning**   | Dynamically trigger tools like `classify_transcription` or `find_local_provider` |\n| **Multi-Turn Chatbot with Memory** | Maintain and reason over evolving patient summaries               |\n\n---\n\n## üß± Project Structure\n\n### **Phase 1: Fine-Tuning a Model for Medical Classification**\n\n- **1.** Load dependencies\n- **2.** Prepare and clean medical transcription dataset\n- **3.** Prompt-based zero-shot classification (baseline)\n- **4.** Evaluate predictions using embeddings\n- **5.** Fine-tune Gemini model\n- **6.** Compare and validate performance\n\n### **Phase 2: Building a LangGraph Chatbot**\n\n- **1.** Load and configure environment\n- **2.** Define MedBot state and welcome logic\n- **3.** Add human interaction and looping\n- **4.** Integrate classification tool (tuned/baseline)\n- **5.** Route classification responses and follow-up tools\n- **6.** Add simulated ground search for provider lookup\n- **7.** Plan retrieval tool for similar case search\n\n---\n\n## ‚úÖ Capstone Alignment\n\nThis project satisfies core capstone objectives:\n\n- ‚úÖ Use of **LLM APIs** and prompt engineering\n- ‚úÖ Application of **fine-tuning and zero-shot comparison**\n- ‚úÖ Creation of an **interactive GenAI system** using LangGraph\n- ‚úÖ Clear **modular logic** with multi-tool orchestration\n- ‚úÖ Strong **domain-specific use case** in healthcare\n\n---\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Load dependencies","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:47:32.888978Z","iopub.execute_input":"2025-04-16T21:47:32.889384Z","iopub.status.idle":"2025-04-16T21:47:39.132268Z","shell.execute_reply.started":"2025-04-16T21:47:32.889360Z","shell.execute_reply":"2025-04-16T21:47:39.130474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:47:39.134845Z","iopub.execute_input":"2025-04-16T21:47:39.135954Z","iopub.status.idle":"2025-04-16T21:47:39.144414Z","shell.execute_reply.started":"2025-04-16T21:47:39.135905Z","shell.execute_reply":"2025-04-16T21:47:39.143303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:47:42.274175Z","iopub.execute_input":"2025-04-16T21:47:42.274537Z","iopub.status.idle":"2025-04-16T21:47:42.576383Z","shell.execute_reply.started":"2025-04-16T21:47:42.274515Z","shell.execute_reply":"2025-04-16T21:47:42.575121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explore available models\n\nYou will be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API method to start the fine-tuning job and create your custom model. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python).","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:47:48.274239Z","iopub.execute_input":"2025-04-16T21:47:48.274669Z","iopub.status.idle":"2025-04-16T21:47:48.548028Z","shell.execute_reply.started":"2025-04-16T21:47:48.274640Z","shell.execute_reply":"2025-04-16T21:47:48.546753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Dataset Preparation & Label Cleaning\n\n### üìÅ Dataset: Medical Transcriptions\n\nThe [**Medical Transcriptions**](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions) dataset has been added to this notebook as an input under the **`/kaggle/input/medicaltranscriptions`** directory.\n\nIt contains a CSV file with transcribed medical records across various specialties and report types. This dataset can be used for tasks such as:\n\n- **Text classification** (e.g., predicting medical specialty from transcription text)\n- **Named Entity Recognition (NER)** for extracting patient symptoms, medications, or diagnoses\n- **Fine-tuning language models** for medical domain understanding\n\n#### üìÑ File Structure\n- `medicaltranscriptions.csv`: The primary dataset file containing the transcribed medical reports.\n\n#### üßæ Key Columns\n- `Medical Specialty`: The category of the transcription (e.g., Cardiology, Radiology).\n- `Sample Name`: A brief title or label for the transcription.\n- `Transcription`: The full text of the medical report.\n\nWe will use this dataset to fine-tune a custom language model for classification tasks.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/medicaltranscriptions/mtsamples.csv', index_col=0)\n\n\n# Get the full sorted list of unique sample_name values\nmedical_specialty_list = sorted(df['medical_specialty'].dropna().unique())\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:47:52.505982Z","iopub.execute_input":"2025-04-16T21:47:52.506450Z","iopub.status.idle":"2025-04-16T21:47:52.756204Z","shell.execute_reply.started":"2025-04-16T21:47:52.506417Z","shell.execute_reply":"2025-04-16T21:47:52.755078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['transcription'])\n\nvalid_specialties= [' Urology', ' Nephrology',]\n# Step 3: Filter the DataFrame to keep only those rows\ndf = df[df['medical_specialty'].isin(valid_specialties)]\n\nprint(df.shape)\n\ndf['medical_specialty'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:15.640728Z","iopub.execute_input":"2025-04-16T21:48:15.641096Z","iopub.status.idle":"2025-04-16T21:48:15.658654Z","shell.execute_reply.started":"2025-04-16T21:48:15.641072Z","shell.execute_reply":"2025-04-16T21:48:15.657535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport re\n\ndef preprocess_transcription(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    return text.strip()[:40000]\n\n\n\ndf['textInput'] = df['transcription'].apply(preprocess_transcription)\ndf['output'] = df['medical_specialty']\ndf_tune = df[['textInput', 'output']]\ntune_data = {'examples': df_tune.to_dict(orient='records')}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:20.430388Z","iopub.execute_input":"2025-04-16T21:48:20.430807Z","iopub.status.idle":"2025-04-16T21:48:20.491800Z","shell.execute_reply.started":"2025-04-16T21:48:20.430777Z","shell.execute_reply":"2025-04-16T21:48:20.490619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_data(df, num_samples):\n    return (\n        df.groupby(\"output\", group_keys=False)\n          .apply(lambda x: x.sample(min(len(x), num_samples), random_state=42))\n          .reset_index(drop=True)\n    )\n\n\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 20\n\ndf_train = sample_data(df, TRAIN_NUM_SAMPLES)\ndf_test = sample_data(df, TEST_NUM_SAMPLES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:24.801366Z","iopub.execute_input":"2025-04-16T21:48:24.802230Z","iopub.status.idle":"2025-04-16T21:48:24.827557Z","shell.execute_reply.started":"2025-04-16T21:48:24.802189Z","shell.execute_reply":"2025-04-16T21:48:24.825747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_idx = 0\nsample_row = df_test.iloc[sample_idx]['textInput']\nsample_label = df_test.iloc[sample_idx]['output']\n\nprint(sample_row)\nprint('---')\nprint('Actual Label:', sample_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:27.737780Z","iopub.execute_input":"2025-04-16T21:48:27.738179Z","iopub.status.idle":"2025-04-16T21:48:27.745585Z","shell.execute_reply.started":"2025-04-16T21:48:27.738153Z","shell.execute_reply":"2025-04-16T21:48:27.744058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Prompt Engineering Baseline Model\nIn this step, we evaluated the capabilities of Gemini models for medical transcription classification using prompt engineering. The goal was to classify each transcription into a medical specialty (e.g., Urology or Nephrology) without any fine-tuning.\n","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Zero-shot Prompt Engineering\nWe first tested Gemini using a **na√Øve zero-shot prompt**, directly asking the model what category a transcription belongs to:","metadata":{}},{"cell_type":"code","source":"# Ask the model directly in a zero-shot prompt.\nprompt = \"What category does the following medical transcription belong to?\"\n\nresponse = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\",\n    contents=[prompt, sample_row]\n)\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:30.208312Z","iopub.execute_input":"2025-04-16T21:48:30.208739Z","iopub.status.idle":"2025-04-16T21:48:31.155821Z","shell.execute_reply.started":"2025-04-16T21:48:30.208711Z","shell.execute_reply":"2025-04-16T21:48:31.154696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This initial approach helped assess whether the model could infer medical context from raw clinical notes without prior task definition. While it sometimes returned correct responses, the output lacked consistency and structure.","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Prompt-Engineered Function (with System Instruction)\nTo improve reliability, we crafted a structured prompt using Gemini's system_instruction feature. This clarified the model‚Äôs role as a classification service. We then wrapped this in a callable prediction function.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\nfrom google.genai import types\n\n# Define system instruction for classification\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na medical transcription,and you must respond with the category it belongs to.\n\"\"\"\n\n# Retry handler for rate limits or service unavailability\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable)\ndef predict_label(text: str) -> str:\n    response = client.models.generate_content(\n        model=\"gemini-1.5-flash-001\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=text)\n\n    rc = response.candidates[0]\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        return rc.content.parts[0].text.strip()\n\n\n\nprediction = predict_label(sample_row)\n\nprint(\"Prediction:\", prediction)\nprint(\"Actual:\", sample_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:35.304449Z","iopub.execute_input":"2025-04-16T21:48:35.305529Z","iopub.status.idle":"2025-04-16T21:48:35.588103Z","shell.execute_reply.started":"2025-04-16T21:48:35.305492Z","shell.execute_reply":"2025-04-16T21:48:35.586476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\ntqdmr.pandas()  # üîß Activate tqdm for Pandas\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n\n# Re-sample 2 examples per class from df_test\ndef sample_data(df, num_samples):\n    return (\n        df.groupby(\"output\", group_keys=False)\n          .apply(lambda x: x.sample(min(len(x), num_samples), random_state=42))\n          .reset_index(drop=True)\n    )\n\ndf_baseline_eval = sample_data(df_test, 10)\n\n\n## predict ocross the test set\ndf_baseline_eval['Prediction'] = df_baseline_eval['textInput'].progress_apply(predict_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:42.638872Z","iopub.execute_input":"2025-04-16T21:48:42.639241Z","iopub.status.idle":"2025-04-16T21:48:52.049092Z","shell.execute_reply.started":"2025-04-16T21:48:42.639217Z","shell.execute_reply":"2025-04-16T21:48:52.047967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Embedding-Based Evaluation\n\n### 4.1 Define Embedding Function (Using Gemini)\nWhat we‚Äôre doing:\n\n\nWe will define a retryable function that uses text-embedding-004 to generate an embedding vector for a given string. This is used to semantically compare category names.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\nfrom google.genai import types\n\n# Retry handler for rate limits or temporary issues\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300.0)\ndef embed_fn(text: str) -> list[float]:\n    \"\"\"Get Gemini embedding for text (for classification).\"\"\"\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",\n        contents=text,\n        config=types.EmbedContentConfig(\n            task_type=\"classification\",  # We're using this for label similarity\n        ),\n    )\n    return response.embeddings[0].values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:52.054791Z","iopub.execute_input":"2025-04-16T21:48:52.055053Z","iopub.status.idle":"2025-04-16T21:48:52.062267Z","shell.execute_reply.started":"2025-04-16T21:48:52.055033Z","shell.execute_reply":"2025-04-16T21:48:52.060971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 Create Embeddings for Predicted and Actual Labels\nüìåWhat we‚Äôre doing:\nApply embed_fn() to df_baseline_eval[\"output\"] and [\"Prediction\"].\n\nStore embeddings in new columns for later comparison.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm on pandas\ntqdmr.pandas()\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Generate embeddings\ndf_baseline_eval[\"Actual_Embed\"] = df_baseline_eval[\"output\"].progress_apply(embed_fn)\ndf_baseline_eval[\"Predicted_Embed\"] = df_baseline_eval[\"Prediction\"].progress_apply(embed_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:52.063786Z","iopub.execute_input":"2025-04-16T21:48:52.064089Z","iopub.status.idle":"2025-04-16T21:48:59.153476Z","shell.execute_reply.started":"2025-04-16T21:48:52.064068Z","shell.execute_reply":"2025-04-16T21:48:59.152404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ### 4.3 Calculate Cosine Similarity\nüìå What we're doing:\nWe‚Äôll compute the cosine similarity between the predicted and actual label embeddings for each row. This gives us a numerical score (0‚Äì1) for how semantically close the predicted label is to the ground truth.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef compute_similarity(row):\n    \"\"\"Compute cosine similarity between predicted and actual embeddings.\"\"\"\n    actual_vec = np.array(row[\"Actual_Embed\"]).reshape(1, -1)\n    predicted_vec = np.array(row[\"Predicted_Embed\"]).reshape(1, -1)\n    return cosine_similarity(actual_vec, predicted_vec)[0][0]\n\n# Apply similarity computation\ndf_baseline_eval[\"Similarity\"] = df_baseline_eval.apply(compute_similarity, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:59.162997Z","iopub.execute_input":"2025-04-16T21:48:59.163324Z","iopub.status.idle":"2025-04-16T21:48:59.187942Z","shell.execute_reply.started":"2025-04-16T21:48:59.163291Z","shell.execute_reply":"2025-04-16T21:48:59.186848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.4 Define Matching Based on Similarity Threshold\nüìå What we're doing:\nInstead of checking if the labels are exactly the same, we‚Äôll say a prediction is ‚ÄúCorrect‚Äù if similarity is above a threshold ‚Äî say 0.8 (you can tune this later).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# --- Threshold for match ---\nSIMILARITY_THRESHOLD = 0.8\ndf_baseline_eval[\"Match\"] = df_baseline_eval[\"Similarity\"] >= SIMILARITY_THRESHOLD\n\n# --- Step 1: Semantic accuracy ---\nsemantic_accuracy = df_baseline_eval[\"Match\"].mean()\nprint(f\"üß† Embedding-based Semantic Accuracy: {semantic_accuracy:.2%}\")\n\n# --- Step 2: Confusion matrix with similarity values ---\nconfusion_df = (\n    df_baseline_eval.groupby([\"output\", \"Prediction\"])[\"Similarity\"]\n    .mean()\n    .unstack(fill_value=0)\n)\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(confusion_df, annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=0.5)\nplt.title(\"üîç Confusion Matrix (Average Embedding Similarity)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n# --- Step 3: Precision / Recall / F1 based on semantic match ---\n# Binary classification (Match = Correct)\nprecision = precision_score(df_baseline_eval[\"Match\"], [True]*len(df_baseline_eval))\nrecall = recall_score(df_baseline_eval[\"Match\"], [True]*len(df_baseline_eval))\nf1 = f1_score(df_baseline_eval[\"Match\"], [True]*len(df_baseline_eval))\n\nprint(f\"üìä Precision: {precision:.2%}\")\nprint(f\"üìä Recall:    {recall:.2%}\")\nprint(f\"üìä F1 Score:  {f1:.2%}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:48:59.193192Z","iopub.execute_input":"2025-04-16T21:48:59.194011Z","iopub.status.idle":"2025-04-16T21:48:59.662442Z","shell.execute_reply.started":"2025-04-16T21:48:59.193982Z","shell.execute_reply":"2025-04-16T21:48:59.661329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Manual Evaluation for Individual Sample**","metadata":{}},{"cell_type":"code","source":"sample_idx = 1\n\nrow = df_baseline_eval.iloc[sample_idx]\ntext = row[\"textInput\"]\nactual = row[\"output\"]\npredicted = row[\"Prediction\"]\nsimilarity = row[\"Similarity\"]\nmatch = row[\"Match\"]\n\nprint(f\"üìù Transcription:\\n{text[:500]}...\\n\")  # Truncated for readability\nprint(f\"‚úÖ Actual Label:     {actual}\")\nprint(f\"ü§ñ Predicted Label:  {predicted}\")\nprint(f\"üìê Cosine Similarity: {similarity:.3f}\")\nprint(\"üéØ Match:\", \"‚úÖ Correct\" if match else \"‚ùå Incorrect\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:49:03.772171Z","iopub.execute_input":"2025-04-16T21:49:03.772520Z","iopub.status.idle":"2025-04-16T21:49:03.782141Z","shell.execute_reply.started":"2025-04-16T21:49:03.772495Z","shell.execute_reply":"2025-04-16T21:49:03.780727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Fine-Tuning and Model Comparison\n\n","metadata":{}},{"cell_type":"markdown","source":"### 5.1 üîß Fine-Tuning Gemini with Medical Transcription Data\n\nIn this step, we fine-tune the `gemini-1.5-flash-001` model using our medical transcription dataset.\n\nWe're using Google's **parameter-efficient fine-tuning (PEFT)** approach. This technique updates only a small number of model parameters (adapters), making training faster and more resource-efficient, while still enabling the model to adapt to our specific task ‚Äî in this case, classifying medical transcriptions by specialty.\n\n#### üìå Key Parameters:\n- **Base model**: `gemini-1.5-flash-001-tuning` ‚Äî the fine-tunable version of Gemini Flash.\n- **Training data**: A JSON-style dictionary containing a list of `{\"textInput\", \"output\"}` pairs.\n- **Batch size**: 16 ‚Äî number of samples processed together during each step.\n- **Epochs**: 2 ‚Äî each sample will be seen twice during training (for quick testing, can increase later).\n\nOnce the tuning job is submitted, we store the `model_id` so we can track and later evaluate or use the tuned model.\n\n_Note: Tuning can take a few minutes to over an hour depending on load, so be patient or use a previously tuned model if available._\n","metadata":{}},{"cell_type":"code","source":"# from google import genai\n# from google.genai import types\n\ntune_op = client.tunings.tune(\n    base_model=\"models/gemini-1.5-flash-001-tuning\",\n    training_dataset=tune_data,\n    config=types.CreateTuningJobConfig(\n        tuned_model_display_name=\"medical-text-classifier\",\n        batch_size=16,\n        epoch_count=2,  # start low for test run\n    ),\n)\nmodel_id = tune_op.name\nprint(\"Tuning started:\", model_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:08:47.111005Z","iopub.execute_input":"2025-04-15T18:08:47.111383Z","iopub.status.idle":"2025-04-15T18:08:47.822353Z","shell.execute_reply.started":"2025-04-15T18:08:47.111358Z","shell.execute_reply":"2025-04-15T18:08:47.821415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Monitor Progress","metadata":{}},{"cell_type":"code","source":"# model_id = \"tunedModels/medicaltranscriptionclassifier-pnwuvgdln\" ## data had so many labels\nmodel_id = \"tunedModels/medicaltextclassifier-matn7qfwnz0j\" ## data had two labels 'urology' and 'nephrology'\nmodel_status = client.tunings.get(name=model_id)\nprint(model_status.state)\n# client.tunings.get(name=model_id).state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:49:13.872668Z","iopub.execute_input":"2025-04-16T21:49:13.873042Z","iopub.status.idle":"2025-04-16T21:49:14.414902Z","shell.execute_reply.started":"2025-04-16T21:49:13.873011Z","shell.execute_reply":"2025-04-16T21:49:14.413676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 Use Your Tuned Model for Prediction\nYou just need to update your predict_label() function to call the tuned model.\nAnd run Predictions on df_test","metadata":{}},{"cell_type":"code","source":"# model_id = \"tunedModels/medicaltranscriptionclassifier-pnwuvgdln\" ## data had so many labels\nmodel_id = \"tunedModels/medicaltextclassifier-matn7qfwnz0j\" ## data had two labels 'urology' and 'nephrology'\nTUNED_MODEL_ID = model_id  # Replace with your actual ID\n\n@retry.Retry(predicate=is_retriable)\ndef predict_label_tuned(text: str) -> str:\n    response = client.models.generate_content(\n        model=TUNED_MODEL_ID,\n        contents=text\n    )\n    rc = response.candidates[0]\n    return rc.content.parts[0].text.strip() if rc.finish_reason.name == \"STOP\" else \"(error)\"\n\n\ndf_tuned_eval = df_test.copy()\ndf_tuned_eval[\"Prediction\"] = df_tuned_eval[\"textInput\"].progress_apply(predict_label_tuned)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:49:20.073953Z","iopub.execute_input":"2025-04-16T21:49:20.074318Z","iopub.status.idle":"2025-04-16T21:52:11.546710Z","shell.execute_reply.started":"2025-04-16T21:49:20.074297Z","shell.execute_reply":"2025-04-16T21:52:11.545568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.3 Embedding-Based Semantic Evaluation\n\nWe used vector embeddings to measure how closely the predicted labels align with the true labels. The workflow includes:\n\n- Step 1: Embed Actual and Predicted Labels\n- Step 2: Compute Cosine Similarity\n- Step 3: Define a Match Threshold\n- Step 4: Semantic Accuracy Score","metadata":{}},{"cell_type":"code","source":"\ndf_tuned_eval[\"Actual_Embed\"] = df_tuned_eval[\"output\"].progress_apply(embed_fn)\ndf_tuned_eval[\"Predicted_Embed\"] = df_tuned_eval[\"Prediction\"].progress_apply(embed_fn)\n\n\n# Calculate Similarity Scores\ndf_tuned_eval[\"Similarity\"] = df_tuned_eval.apply(compute_similarity, axis=1)\n\n\n# Define Match by Similarity Threshold\nSIMILARITY_THRESHOLD = 0.8\ndf_tuned_eval[\"Match\"] = df_tuned_eval[\"Similarity\"] >= SIMILARITY_THRESHOLD\n\n\n# Semantic Accuracy\nsemantic_accuracy = df_tuned_eval[\"Match\"].mean()\nprint(f\"üß† Tuned Model Semantic Accuracy: {semantic_accuracy:.2%}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:52:11.553376Z","iopub.execute_input":"2025-04-16T21:52:11.553754Z","iopub.status.idle":"2025-04-16T21:52:25.647641Z","shell.execute_reply.started":"2025-04-16T21:52:11.553726Z","shell.execute_reply":"2025-04-16T21:52:25.646462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**üìä Confusion Matrix with Similarity Weights**\n\n\nWe also generate a confusion matrix that shows average cosine similarity between each actual vs. predicted label pair:","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix Heatmap (Similarity-Weighted)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nconfusion_df = (\n    df_tuned_eval.groupby([\"output\", \"Prediction\"])[\"Similarity\"]\n    .mean()\n    .unstack(fill_value=0)\n)\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(confusion_df, annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=0.5)\nplt.title(\"üîç Tuned Model Confusion Matrix (Embedding Similarity)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:52:25.649524Z","iopub.execute_input":"2025-04-16T21:52:25.649880Z","iopub.status.idle":"2025-04-16T21:52:25.941981Z","shell.execute_reply.started":"2025-04-16T21:52:25.649857Z","shell.execute_reply":"2025-04-16T21:52:25.940762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Precision, Recall, F1 (Semantic Match as Proxy for Correctness)\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(df_tuned_eval[\"Match\"], [True]*len(df_tuned_eval))\nrecall = recall_score(df_tuned_eval[\"Match\"], [True]*len(df_tuned_eval))\nf1 = f1_score(df_tuned_eval[\"Match\"], [True]*len(df_tuned_eval))\n\nprint(f\"üìä Precision: {precision:.2%}\")\nprint(f\"üìä Recall:    {recall:.2%}\")\nprint(f\"üìä F1 Score:  {f1:.2%}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:52:25.943139Z","iopub.execute_input":"2025-04-16T21:52:25.943438Z","iopub.status.idle":"2025-04-16T21:52:25.966228Z","shell.execute_reply.started":"2025-04-16T21:52:25.943415Z","shell.execute_reply":"2025-04-16T21:52:25.964818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Compare and validate the Tuned Model","metadata":{}},{"cell_type":"markdown","source":"Here are some challenging test examples ‚Äî intentionally ambiguous between Nephrology (kidneys, renal function) and Urology (urinary tract, bladder, prostate). \n\nThese are crafted to make it tricky even for humans:","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\nfrom google.genai import types\nimport pandas as pd\n\n# Define model IDs\nBASELINE_MODEL_ID = \"models/gemini-1.5-flash-001\"\nTUNED_MODEL_ID = \"tunedModels/medicaltextclassifier-matn7qfwnz0j\"\n\n\n# Define system instruction for the baseline model for classification\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na medical transcription,\nand you must respond with the category it belongs to.\n\"\"\"\n\n\n\n\n# Retry handler\nis_retriable = lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503}\n\n@retry.Retry(predicate=is_retriable)\ndef get_prediction(model_id, prompt_text):\n    if model_id == BASELINE_MODEL_ID:\n        response = client.models.generate_content(\n            model=model_id,\n            contents=prompt_text,\n            config=types.GenerateContentConfig(\n                system_instruction=system_instruct\n            )\n        )\n    else:\n        # For tuned models, no need for system prompt\n        response = client.models.generate_content(\n            model=model_id,\n            contents=prompt_text\n        )\n    \n    return response.text.strip()\n\n\n# Define prompts and actual labels\ntest_cases = [\n    {\n        \"Prompt\": \"The patient presents with recurrent flank pain and hematuria. Imaging showed a 7 mm calculus...\",\n        \"FullPrompt\": \"The patient presents with recurrent flank pain and hematuria. Imaging showed a 7 mm calculus in the left ureter near the vesicoureteral junction. Creatinine levels are slightly elevated. History of hypertension and type 2 diabetes.\",\n        \"Actual\": \"nephrology\"\n    },\n    {\n        \"Prompt\": \"Patient reports difficulty initiating urination, dribbling, and mild lower back pain...\",\n        \"FullPrompt\": \"Patient reports difficulty initiating urination, dribbling, and mild lower back pain. PSA levels normal. No signs of infection. Ultrasound indicates mild hydronephrosis and possible bladder outlet obstruction.\",\n        \"Actual\": \"urology\"\n    },\n    {\n        \"Prompt\": \"A 58-year-old male with a history of chronic kidney disease stage 3, presents with urgency...\",\n        \"FullPrompt\": \"A 58-year-old male with a history of chronic kidney disease stage 3, presents with urgency and frequency. Urinalysis shows microalbuminuria and trace blood. Renal ultrasound normal. Referred for urologic evaluation.\",\n        \"Actual\": \"nephrology\"\n    },\n]\n\n# Collect results\nresults = []\n\nfor case in test_cases:\n    baseline_pred = get_prediction(BASELINE_MODEL_ID, case[\"FullPrompt\"])\n    tuned_pred = get_prediction(TUNED_MODEL_ID, case[\"FullPrompt\"])\n    \n    results.append({\n        \"Prompt (abbreviated)\": case[\"Prompt\"],\n        \"Actual\": case[\"Actual\"],\n        \"Baseline Prediction\": baseline_pred,\n        \"Baseline Match\": \"‚úÖ\" if baseline_pred.lower() == case[\"Actual\"].lower() else \"‚ùå\",\n        \"Tuned Prediction\": tuned_pred,\n        \"Tuned Match\": \"‚úÖ\" if tuned_pred.lower() == case[\"Actual\"].lower() else \"‚ùå\",\n    })\n\n# Convert to DataFrame and show as table\ndf_results = pd.DataFrame(results)\nimport IPython.display as display\ndisplay.display(df_results)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:53:13.274765Z","iopub.execute_input":"2025-04-16T21:53:13.275158Z","iopub.status.idle":"2025-04-16T21:53:16.205222Z","shell.execute_reply.started":"2025-04-16T21:53:13.275131Z","shell.execute_reply":"2025-04-16T21:53:16.204095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# ü§ñ Phase 2: MedBot ‚Äì LangGraph Chatbot for Medical Transcription Classification\n\nThis phase focuses on building an interactive medical assistant chatbot named **MedBot** using [LangGraph](https://www.langchain.com/langgraph). MedBot is designed to assist users with **nephrology** and **urology** cases by classifying user-provided clinical summaries.\n\n---\n\n### üß† What MedBot Can Do (Current Capabilities)\n\n- **Conversational Input Handling**: MedBot chats naturally with users to collect patient symptoms, diagnoses, and clinical details.\n- **Medical Classification**: Once enough information is gathered, MedBot uses a **Gemini 1.5 model** (prompt-engineered or fine-tuned) to classify the case into one of the following:\n  - **Nephrology**\n  - **Urology**\n  - **Other**\n- **Scoped Dialogue Management**: If the case is outside its scope (e.g., classified as \"Other\"), it politely redirects the user and suggests consulting a provider.\n\n---\n\n### üõ†Ô∏è GenAI Techniques Used\n\n- **Prompt Engineering**: Clear system instructions guide MedBot‚Äôs tone, behavior, and tool invocation.\n- **LangGraph Modular Design**:\n  - **State management** for conversation history and tool results.\n  - **Conditional routing** between nodes like the chatbot, tool classifier, and human.\n- **LLM Tool Invocation**: Tools (like `classify_transcription`) are triggered based on user input patterns.\n\n---\n\n### ‚úÖ Summary\n\nThis modular chatbot architecture serves as a foundation for a more intelligent medical assistant system. It demonstrates structured LLM interaction with LangGraph, real-time tool invocation, and safe domain-specific communication‚Äîall key requirements for a modern GenAI assistant in healthcare support.\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"We need to restart the kernell and start running from here again.\n\nAt this point we have everything that we have:\n- embedded transcriptions of all samples with 'Urology' or 'Nephrology' Category.\n- fine-tuned model that classify any given text.\n- original transcription data.","metadata":{}},{"cell_type":"markdown","source":"## 1. Load and Configure Environment\n\nThis step loads:\n- The tuned Gemini model ID\n- The dataframe with **20 samples per class**\n- Precomputed Gemini embeddings for retrieval\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:16:27.017831Z","iopub.execute_input":"2025-04-17T15:16:27.018176Z","iopub.status.idle":"2025-04-17T15:16:27.217523Z","shell.execute_reply.started":"2025-04-17T15:16:27.018153Z","shell.execute_reply":"2025-04-17T15:16:27.216636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove conflicting packages from the Kaggle base environment.\n!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n# Install langgraph and the packages used in this lab.\n!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:16:31.572742Z","iopub.execute_input":"2025-04-17T15:16:31.573056Z","iopub.status.idle":"2025-04-17T15:17:00.054656Z","shell.execute_reply.started":"2025-04-17T15:16:31.573031Z","shell.execute_reply":"2025-04-17T15:17:00.053751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport google as genai\nfrom google.api_core import retry\nfrom google.genai import types\nfrom google.api_core import exceptions\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom google.api_core import retry\n\n\n# Set model IDs\nTUNED_MODEL_ID = \"tunedModels/medicaltextclassifier-matn7qfwnz0j\"\nEMBEDDING_MODEL = \"models/text-embedding-004\"\n\n\n## embeddings of all transcriptions from Urology or Nephrology class\ndf = pd.read_csv('/kaggle/input/dembeddings-all-uro-nephro-logy-transcriptions/df_with_embeddings_all_uro_nephro_logy_transcriptions.csv', index_col=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:17:03.117591Z","iopub.execute_input":"2025-04-17T15:17:03.117931Z","iopub.status.idle":"2025-04-17T15:17:03.173165Z","shell.execute_reply.started":"2025-04-17T15:17:03.117903Z","shell.execute_reply":"2025-04-17T15:17:03.172252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Define MedBot State and Welcome Message\n\nThis state definition uses `TypedDict` and LangGraph's `add_messages` annotation to preserve message history across conversation turns. We also define the initial welcome message the bot will display when it starts.\n\nThis step sets up:\n- `MedState`: conversation state\n- `WELCOME_MSG`: opening line from MedBot\n","metadata":{}},{"cell_type":"code","source":"from typing_extensions import TypedDict\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages.ai import AIMessage\n\n\n# ‚úÖ MedBot state definition (inspired by BaristaBot)\nclass MedState(TypedDict, total=False):\n    messages: Annotated[list, add_messages]\n    predicted_category: str\n    retrieved_case: dict\n    ground_response: str\n    finished: bool\n\n# ‚úÖ System instruction for MedBot\nMEDBOT_SYSINT = (\n    \"system\",\n    \"You are MedBot, a specialized assistant for nephrology and urology cases.\\n\\n\"\n    \"When the user provides a case description, ALWAYS call the `classify_transcription` tool \"\n    \"with their message to determine if the issue is related to nephrology, urology, or something else.\\n\\n\"\n    \"Only after calling the tool and receiving the classification, respond to the user appropriately:\\n\"\n    \"- If the issue is classified as urology or nephrology, say that it seems related to that field, and offer to retrieve similar cases or accept more details.\\n\"\n    \"- If the issue is classified as 'Other', say that it's outside your scope and offer to help find a provider nearby.\\n\\n\"\n    \"Do not guess the category yourself ‚Äî always invoke the `classify_transcription` tool first.\\n\\n\"\n    \"You must call the tool every time the user describes a patient case.\"\n)\n\n\n\n# ‚úÖ Updated welcome message\nWELCOME_MSG = \"ü©∫ MedBot is ready. Describe a patient case related to nephrology or urology (or type 'exit' to quit).\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:17:09.642511Z","iopub.execute_input":"2025-04-17T15:17:09.643581Z","iopub.status.idle":"2025-04-17T15:17:10.760186Z","shell.execute_reply.started":"2025-04-17T15:17:09.643515Z","shell.execute_reply":"2025-04-17T15:17:10.759397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Define the MedBot Chat Node and Build the Initial Graph\n\nIn this step, we implement the core chatbot logic for a single conversational turn. The LangGraph graph will start with this chatbot node and terminate afterward (we‚Äôll add human input and looping in the next step).\n\n- `medbot()` uses the Gemini model to respond to messages in `state[\"messages\"]`\n- Messages are prepended with the `MEDBOT_SYSINT` system instruction\n- The graph starts from `START`, runs the chatbot node, and ends at `END`\n","metadata":{}},{"cell_type":"code","source":"from langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Gemini LLM (same as BaristaBot)\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n\ndef medbot(state: MedState) -> MedState:\n    \"\"\"MedBot logic for now is just to respond using the Gemini model.\"\"\"\n    history = [MEDBOT_SYSINT] + state[\"messages\"]\n    return {\"messages\": [llm.invoke(history)]}\n\n# Set up initial graph\ngraph_builder = StateGraph(MedState)\ngraph_builder.add_node(\"medbot\", medbot)\ngraph_builder.add_edge(START, \"medbot\")\nmed_graph = graph_builder.compile()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:06.349217Z","iopub.execute_input":"2025-04-17T17:43:06.350073Z","iopub.status.idle":"2025-04-17T17:43:06.358861Z","shell.execute_reply.started":"2025-04-17T17:43:06.350045Z","shell.execute_reply":"2025-04-17T17:43:06.358064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\nImage(med_graph.get_graph().draw_mermaid_png())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:17:18.687038Z","iopub.execute_input":"2025-04-17T15:17:18.687329Z","iopub.status.idle":"2025-04-17T15:17:18.763335Z","shell.execute_reply.started":"2025-04-17T15:17:18.687292Z","shell.execute_reply":"2025-04-17T15:17:18.762600Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"‚úÖ You can now test a single-turn interaction with:","metadata":{}},{"cell_type":"code","source":"state = med_graph.invoke({\"messages\": ['Hello']})\nfor msg in state[\"messages\"]:\n    print(f\"{type(msg).__name__}: {msg.content}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:23.528793Z","iopub.execute_input":"2025-04-17T17:43:23.529060Z","iopub.status.idle":"2025-04-17T17:43:24.077335Z","shell.execute_reply.started":"2025-04-17T17:43:23.529042Z","shell.execute_reply":"2025-04-17T17:43:24.076558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Add Human Node and Interaction Loop\n\nWe add a `human` node that:\n\n- Displays the assistant's last message.\n- Collects user input from the command line.\n- Terminates if the user types an exit command like `exit` or `q`.\n\nWe also update the MedBot node to send a welcome message if it's the start of the conversation.\n","metadata":{}},{"cell_type":"code","source":"# Human node: responds to MedBot and allows input\ndef human_node(state: MedState) -> MedState:\n    last_msg = state[\"messages\"][-1]\n    print(\"ü§ñ MedBot:\", last_msg.content)\n    \n    user_input = input(\"üë§ You: \")\n\n    if user_input.lower().strip() in {\"exit\", \"quit\", \"q\"}:\n        state[\"finished\"] = True\n\n    return state | {\"messages\": [(\"user\", user_input)]}\n\n# Enhanced MedBot: sends welcome message if no prior messages\ndef medbot_with_welcome(state: MedState) -> MedState:\n    if state[\"messages\"]:\n        output = llm.invoke([MEDBOT_SYSINT] + state[\"messages\"])\n    else:\n        output = {\"content\": \"ü©∫ MedBot is ready. Describe a patient case related to nephrology or urology (or type 'exit' to quit).\"}\n    return state | {\"messages\": [output]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:26.376609Z","iopub.execute_input":"2025-04-17T17:43:26.377147Z","iopub.status.idle":"2025-04-17T17:43:26.383218Z","shell.execute_reply.started":"2025-04-17T17:43:26.377124Z","shell.execute_reply":"2025-04-17T17:43:26.382488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Define Routing Logic and Loop the Conversation\n\nWe define a conditional edge:\n- If the user says \"exit\", go to END.\n- Otherwise, loop back to MedBot for more interaction.\n","metadata":{}},{"cell_type":"code","source":"from typing import Literal\n\n# Control flow logic: loop or exit\ndef maybe_exit(state: MedState) -> Literal[\"medbot\", \"__end__\"]:\n    return END if state.get(\"finished\") else \"medbot\"\n\n# Build the full graph with loop\ngraph_builder = StateGraph(MedState)\ngraph_builder.add_node(\"medbot\", medbot_with_welcome)\ngraph_builder.add_node(\"human\", human_node)\n\ngraph_builder.add_edge(START, \"medbot\")\ngraph_builder.add_edge(\"medbot\", \"human\")\ngraph_builder.add_conditional_edges(\"human\", maybe_exit)\n\nmedbot_loop_graph = graph_builder.compile()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:29.297694Z","iopub.execute_input":"2025-04-17T17:43:29.297985Z","iopub.status.idle":"2025-04-17T17:43:29.305685Z","shell.execute_reply.started":"2025-04-17T17:43:29.297964Z","shell.execute_reply":"2025-04-17T17:43:29.304978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\nImage(medbot_loop_graph.get_graph().draw_mermaid_png())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:31.658644Z","iopub.execute_input":"2025-04-17T17:43:31.658951Z","iopub.status.idle":"2025-04-17T17:43:31.722816Z","shell.execute_reply.started":"2025-04-17T17:43:31.658928Z","shell.execute_reply":"2025-04-17T17:43:31.722116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Run MedBot**\nNow we can run MedBot in a loop. Type a description of a case, and MedBot will respond.\n\nType `exit` to end the session.\n","metadata":{}},{"cell_type":"code","source":"# Uncomment to run\nstate = medbot_loop_graph.invoke({\"messages\": ['Hi']},\n                                 config={\"recursion_limit\": 50})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:35.061131Z","iopub.execute_input":"2025-04-17T17:43:35.061425Z","iopub.status.idle":"2025-04-17T17:43:49.898003Z","shell.execute_reply.started":"2025-04-17T17:43:35.061402Z","shell.execute_reply":"2025-04-17T17:43:49.897374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Add a Classifier Tool Node\n\nWe define a `classify_transcription` tool.\n\nThis tool will:\n- Use your fine-tuned Gemini model to classify a patient transcription.\n- Predict whether it relates to **Nephrology** or **Urology**.\n\nWe'll wrap this tool using LangGraph's `ToolNode` so it can be invoked automatically when MedBot detects the need.\n","metadata":{}},{"cell_type":"markdown","source":"Define the Tool","metadata":{}},{"cell_type":"code","source":"MEDBOT_SYSINT = (\n    \"system\",\n    \"You are MedBot, an AI assistant that supports patients and providers with nephrology and urology cases only.\\n\\n\"\n\n    \"Here is how you should interact:\\n\\n\"\n\n    \"ü©∫ PHASE 1 ‚Äî INTRODUCTION:\\n\"\n    \"- Greet the user and invite them to describe the patient case.\\n\"\n    \"- Examples: symptoms, diagnoses, labs, imaging findings, etc.\\n\\n\"\n\n    \"üìù PHASE 2 ‚Äî COLLECT HISTORY:\\n\"\n    \"- Keep a running summary of all relevant medical information as `patient_summary`.\\n\"\n    \"- After each message, if the user provides partial information, ask clarifying questions.\\n\"\n    \"- Do NOT call any tools until the user confirms they have shared everything (e.g., says 'that's all', 'no more info', etc).\\n\\n\"\n\n    \"üß† PHASE 3 ‚Äî CLASSIFY:\\n\"\n    \"- Once the patient summary is complete, call the `classify_transcription` tool using the full `patient_summary`.\\n\"\n    \"- Use the exact tool call format below:\\n\"\n    \"```tool_code\\n\"\n    \"classify_transcription(transcription=\\\"...patient_summary...\\\")\\n\"\n    \"```\\n\"\n    \"- Wait for the classification result before continuing.\\n\\n\"\n\n    \"üß≠ PHASE 4 ‚Äî BRANCH BY CATEGORY:\\n\"\n    \"- If classification is `Urology` or `Nephrology`:\\n\"\n    \"  ‚Ä¢ Say: 'Based on your summary, this seems related to [CATEGORY].'\\n\"\n    \"  ‚Ä¢ Ask: 'Would you like me to retrieve a similar case from my documentation?'\\n\"\n    \"  ‚Ä¢ If user says yes, call the future retrieval tool using `patient_summary`.\\n\"\n    \"  ‚Ä¢ After retrieving a case, offer to help them find a local [CATEGORY] specialist.\\n\\n\"\n    \"- If classification is `Other`:\\n\"\n    \"  ‚Ä¢ Say: 'This doesn‚Äôt seem to fall within nephrology or urology.'\\n\"\n    \"  ‚Ä¢ Offer to help them find a general doctor or hospital in their area.\\n\\n\"\n\n    \"üåê PHASE 5 ‚Äî LOCATION HELP:\\n\"\n    \"- If user wants to find help, ask for their ZIP code or city.\\n\"\n    \"- Then call the ground search tool with the location and specialty.\\n\"\n    \"- Present the results politely.\\n\\n\"\n\n    \"üö´ SAFETY AND SCOPE:\\n\"\n    \"- Never provide diagnoses, treatment recommendations, or interpret lab/imaging.\\n\"\n    \"- Always remind the user to consult a real physician for decisions.\\n\"\n    \"- Stay within the nephrology/urology domain at all times.\\n\\n\"\n\n    \"üí¨ TONE:\\n\"\n    \"- Be polite, helpful, concise, and professional.\\n\"\n    \"- Always thank the user for information and invite clarification.\\n\"\n    \"- Avoid hallucinating or guessing ‚Äî stick to instructions and tools only.\\n\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:43:56.801092Z","iopub.execute_input":"2025-04-17T17:43:56.801696Z","iopub.status.idle":"2025-04-17T17:43:56.807071Z","shell.execute_reply.started":"2025-04-17T17:43:56.801670Z","shell.execute_reply":"2025-04-17T17:43:56.806366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing_extensions import TypedDict\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\n\nclass MedState(TypedDict, total=False):\n    messages: Annotated[list, add_messages]\n    predicted_category: str\n    patient_summary: str\n    retrieved_case: dict\n    ground_response: str\n    finished: bool\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:44:00.501484Z","iopub.execute_input":"2025-04-17T17:44:00.502227Z","iopub.status.idle":"2025-04-17T17:44:00.506925Z","shell.execute_reply.started":"2025-04-17T17:44:00.502201Z","shell.execute_reply":"2025-04-17T17:44:00.506084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Your fine-tuned model\nTUNED_MODEL_ID = \"tunedModels/medicaltextclassifier-matn7qfwnz0j\"\nBASELINE_MODEL_ID = \"models/gemini-2.0-flash\"\n\nfrom google.api_core import retry\nfrom google.genai import types\n\n# Classification instruction prompt\nclassification_prompt = \"\"\"\nYou are a classification service. You will be passed input that represents a medical transcription, and you must respond with the category it belongs to.\nValid categories are: Urology, Nephrology, or Other.\nOnly respond with the category name. Do not include explanations.\n\"\"\"\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom pydantic import BaseModel\nfrom langchain_core.tools import tool\n\nclass ClassificationInput(BaseModel):\n    transcription: str\n\n@tool(args_schema=ClassificationInput)\ndef classify_transcription(transcription: str) -> str:\n    \"\"\"Classify transcription as Urology, Nephrology, or Other.\"\"\"\n    return \"placeholder\"\n\n\n\n\nclassifier_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n\n# ‚úÖ Bind your tool so the LLM can return tool_calls\nllm_with_tools = classifier_llm.bind_tools([classify_transcription])\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:35:01.818597Z","iopub.execute_input":"2025-04-17T17:35:01.818902Z","iopub.status.idle":"2025-04-17T17:35:01.832371Z","shell.execute_reply.started":"2025-04-17T17:35:01.818878Z","shell.execute_reply":"2025-04-17T17:35:01.831580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Register the Classifier Tool with LangGraph\nWhat we're doing:\n\nWrap the classify_transcription function into a ToolNode, just like get_menu in BaristaBot.\n\nBind the tool to the Gemini model (llm_with_tools) so it knows the tool exists and can call it.\n\nUpdate your chatbot to use this new llm_with_tools.","metadata":{}},{"cell_type":"markdown","source":" Add Tool Node","metadata":{}},{"cell_type":"code","source":"def classifier_node(state: MedState) -> MedState:\n    print(\"üõ†Ô∏è Classifier node triggered.\")\n    \n    tool_call = state[\"messages\"][-1].tool_calls[0]\n    input_text = tool_call[\"args\"][\"transcription\"]\n\n    # üß† Use the fine-tuned model directly\n    prompt = \"\"\"You are a classification service. \n    You will be passed input that represents a medical transcription, and you must respond with the category it belongs to.\n    Valid categories: Nephrology, Urology, or Other.\n    Only respond with the category name. No explanations.\"\"\"\n    response = llm_with_tools.invoke([(\"system\", prompt), (\"user\", input_text)])\n    label = response.content.strip()\n\n    state[\"predicted_category\"] = label\n\n    if label.lower() in {\"urology\", \"nephrology\"}:\n        reply = (\n            f\"üß† Based on your input, this seems related to **{label}**.\\n\"\n            \"Would you like me to pull up a similar patient transcription, or do you have more clinical details to add?\"\n        )\n    else:\n        reply = (\n            \"üß† This issue doesn‚Äôt seem related to nephrology or urology.\\n\"\n            \"I'm only trained to support those specialties.\\n\"\n            \"Would you like me to help you find a doctor or clinic in your area?\"\n        )\n\n    return state | {\n        \"messages\": [\n            ToolMessage(\n                content=label,\n                tool_call_id=tool_call[\"id\"],\n                name=tool_call[\"name\"]\n            ),\n            AIMessage(content=reply)\n        ]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:35:09.387514Z","iopub.execute_input":"2025-04-17T17:35:09.387947Z","iopub.status.idle":"2025-04-17T17:35:09.394799Z","shell.execute_reply.started":"2025-04-17T17:35:09.387921Z","shell.execute_reply":"2025-04-17T17:35:09.394086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Bind Tools to MedBot, Add Classifier Tool Node to Graph\nWe'll update the graph to include the tool node and routing logic.\n\nTo allow the LLM to call tools like `classify_transcription`, we bind it to the model.\n","metadata":{}},{"cell_type":"code","source":"from langchain_core.messages import AIMessage, ToolMessage\n\ndef medbot_with_tools(state: MedState) -> MedState:\n    defaults = {\n        \"predicted_category\": None,\n        \"retrieved_case\": None,\n        \"ground_response\": None,\n        \"finished\": False\n    }\n    \n    history = state.get(\"messages\", [])\n    \n    # ü©∫ First message? Show welcome\n    if not history:\n        return defaults | state | {\"messages\": [AIMessage(content=WELCOME_MSG)]}\n    \n    # üõ† If previous message was a tool result, interpret it\n    last_msg = history[-1]\n    if isinstance(last_msg, ToolMessage) and last_msg.name == \"classify_transcription\":\n        classification = last_msg.content.strip()\n        state[\"predicted_category\"] = classification\n\n        if classification.lower() in {\"urology\", \"nephrology\"}:\n            reply = (\n                f\"üß† Based on the information, this case seems related to **{classification}**.\\n\"\n                \"Would you like me to retrieve a similar patient transcription from my documentation?\"\n            )\n        else:\n            reply = (\n                \"üß† This doesn‚Äôt seem related to nephrology or urology.\\n\"\n                \"I'm only trained to support those specialties.\\n\"\n                \"Would you like me to help you find a doctor or clinic in your area?\"\n            )\n        return state | {\"messages\": [AIMessage(content=reply)]}\n\n    # üëÇ Otherwise continue conversation and tool use with tool-bound LLM\n    output = llm_with_tools.invoke([MEDBOT_SYSINT] + history)\n    return state | {\"messages\": [output]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:35:13.088929Z","iopub.execute_input":"2025-04-17T17:35:13.089634Z","iopub.status.idle":"2025-04-17T17:35:13.095959Z","shell.execute_reply.started":"2025-04-17T17:35:13.089610Z","shell.execute_reply":"2025-04-17T17:35:13.095167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Literal\n\nimport re\n\n\ndef maybe_route_to_classifier(state: MedState) -> Literal[\"classifier\", \"human\"]:\n    last_msg = state[\"messages\"][-1]\n    return \"classifier\" if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls else \"human\"\n\n\n\n\n\ngraph_builder = StateGraph(MedState)\n\ngraph_builder.add_node(\"medbot\", medbot_with_welcome)\ngraph_builder.add_node(\"human\", human_node)\ngraph_builder.add_node(\"classifier\", classifier_node)\n\ngraph_builder.add_edge(START, \"medbot\")\n\n# üß† Route to tool or user based on tool_calls\ngraph_builder.add_conditional_edges(\"medbot\", maybe_route_to_classifier)\n\n# üõ† Tool always routes back to bot\ngraph_builder.add_edge(\"classifier\", \"medbot\")\n\n# üë§ Human either exits or loops back\ngraph_builder.add_conditional_edges(\"human\", maybe_exit)\n\nmedbot_graph_manual_classifier = graph_builder.compile()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:35:18.605312Z","iopub.execute_input":"2025-04-17T17:35:18.605646Z","iopub.status.idle":"2025-04-17T17:35:18.616274Z","shell.execute_reply.started":"2025-04-17T17:35:18.605621Z","shell.execute_reply":"2025-04-17T17:35:18.615482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\nImage(medbot_graph_manual_classifier.get_graph().draw_mermaid_png())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:35:22.040347Z","iopub.execute_input":"2025-04-17T17:35:22.041029Z","iopub.status.idle":"2025-04-17T17:35:22.097354Z","shell.execute_reply.started":"2025-04-17T17:35:22.041003Z","shell.execute_reply":"2025-04-17T17:35:22.096563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üß™ Sample Prompts for Testing the Chatbot\n**üü¢ Nephrology-related:**\n- \"The patient has elevated creatinine levels and persistent proteinuria over the last two months.\"\n\n- \"Patient presents with chronic kidney disease stage 3 and reports fatigue and edema.\"\n\n- \"Blood work reveals abnormal GFR and microalbuminuria; patient has a history of diabetes and hypertension.\"\n\n**üîµ Urology-related:**\n- \"The patient reports difficulty urinating, lower abdominal pressure, and increased frequency at night.\"\n\n- \"Ultrasound shows an enlarged prostate with post-void residual urine volume of 150 mL.\"\n\n- \"Reports burning sensation during urination, urgency, and a recent UTI treated with antibiotics.\"\n\n**‚ö™ Mixed or borderline case:**\n- \"58-year-old male with hypertension presents with hematuria and mild flank pain; imaging shows mild hydronephrosis.\"\n\n**üî¥ Unrelated (non-nephrology/urology):**\n- \"The patient is experiencing chest tightness and pain radiating to the left arm, especially during exertion.\"\n(Expected: The model might still try to assign one of the two known categories unless specifically trained to say ‚Äúneither.‚Äù)","metadata":{}},{"cell_type":"code","source":"# Uncomment to run\nstate = medbot_graph_manual_classifier.invoke({\"messages\": ['Hi']},\n                                 config={\"recursion_limit\": 50})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:35:28.900024Z","iopub.execute_input":"2025-04-17T17:35:28.900317Z","iopub.status.idle":"2025-04-17T17:37:20.717982Z","shell.execute_reply.started":"2025-04-17T17:35:28.900294Z","shell.execute_reply":"2025-04-17T17:37:20.716912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for msg in state[\"messages\"]:\n    print(type(msg), getattr(msg, \"name\", \"\"), msg.content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:30:49.910397Z","iopub.execute_input":"2025-04-17T17:30:49.911134Z","iopub.status.idle":"2025-04-17T17:30:49.915759Z","shell.execute_reply.started":"2025-04-17T17:30:49.911105Z","shell.execute_reply":"2025-04-17T17:30:49.914938Z"}},"outputs":[],"execution_count":null}]}